
#загрузка библиотек 
```{r}
library(dplyr)
library(ggplot2)
library(stringr) 
library(tidytext)
library(tidyr)
library(stopwords)
library(LDAvis)
library(topicmodels)
library(ldatuning)
library(wordcloud2)
library(quanteda)
library(stm)
```

#загрузка данных 
```{r}
library(readr)
data = read_csv("~/yandex/topic_modeling_data.csv") 
document = c(1:901)
data = cbind(data, document)
data = data %>% dplyr::select(2,3,4,5,9)
```

#explarotary analysis 

какое распределение текстов по нацменам? 
```{r}
data_descr <- data %>% group_by(name) %>% count() %>% arrange(n, decreasing =T) %>% mutate(name = ordered(name, levels = .$name))
ggplot(data_descr) + geom_bar(aes(x=name, y=n),fill="#76EEC6",stat="identity",position="identity") +coord_flip() +theme_bw()+labs(y="Количество абзацев",x="ФИО", title="Распределение абзацев по нацменам")
```

по источникам? 
```{r}
data_source <- data %>% group_by(type) %>% count() %>% arrange(n, decreasing =T) %>% mutate(type = ordered(type, levels = .$type))
ggplot(data_source) + geom_bar(aes(x=type,y=n),fill="#8EE5EE", stat="identity",position="identity") +coord_flip() +theme_bw()+labs(y="Количество",x="Источник", title="Распределение источников")
```
#работа со стоп словами 
```{r}
#мы добавили к стоп-словам слова, которые не имеют ярко выраженного смысла, в основном смотрели самые часто встречающиеся и на те, которые выводились при построении модели 

rustopwords = data.frame(words=c(stopwords::stopwords("ru"), "это", "очень", "россия", "дума", "российский", "законопроект","парламент","чтение","государственный","главный", "федерация", "который", "комитет", "весь","проект","новый","государство","также","часть","поэтому","важный","самый","видеть","наш","ваш","ситуация","свой","метод","позиция","вопрос","данный","безусловно","последний","первый","второй","третий","четвертый", "пятый", "шестой","каждый","несколько","нужно","год","страна","мочь","работа","закон","принимать","должный","работать","решение","народ","время","место","предлагать","хотеть","начинать","субъект","дело","давать","получать","большой","тысяча","вносить","знать","отмечать","сделать","день","понимать","становиться","считать","территория","объем","внимание","касаться","являться","учитывать","считаться","говорить","коллега","миллиард","уважаемый","программа","предложение","законодательство","федеральный","бюджет","идти","круглый","стол","министерство","мэр","иметь","обязательный","количество","думать","необходимо","госдума","далее","находиться","триллион","миллион","хороший","политика","заниматься","огромный","именно","средство","число","фракция","лицо","образ","друг","палата","система","например","кроме","задача","цель","правительство", "рубль","какой","то","какойто","изза", "позволять","создавать","сторона","проблема"), stringsAsFactors=FALSE)
```

```{r}
#удаляем числа и пунктуацию, приводим слова к нижнему регистру.
data$text = str_to_lower(data$text)
punct  <- "[\\.,:;!?\"\'\\()\\-\\%\\—\\«\\»]"
numbers <- "[1,2,3,4,5,6,7,8,9,0]"
data$text = str_remove_all(data$text, punct) 
data$text = str_remove_all(data$text, numbers) 
```

#лемматизация
```{r}
text.lem <- system2("mystem", c("-c", "-l", "-d"), input = data$text, stdout=TRUE) 
```

```{r}
text.lem <- str_replace_all(text.lem, "\\{([^}]+?)([?]+)?\\}", "\\1")
data <- cbind(data, lem = text.lem)
```

#разбивка на токены и группировка по словам
```{r}
data_tokens = data %>%
  unnest_tokens(words, lem) %>% anti_join(rustopwords)

word_counts <- data_tokens %>%
  count(document, words, sort = TRUE) %>%
  ungroup()
```
#построим облако слов 
```{r}
words_cloud = data_tokens %>%
   count(words) %>% arrange(-n) %>% 
    top_n(50, n)

wordcloud2(data = words_cloud)
```


#выберем оптимальное количество тем 
```{r}
data_tm <- word_counts %>%
  cast_dtm(document, words, n)

result <- FindTopicsNumber(
  data_tm,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)
```
#если смотреть на метрики, то можно сказать, что 5 тем - наиболее приемлемое количество тем
```{r}
FindTopicsNumber_plot(result)
```

#построение модели - мы выбрали 5 тем 

```{r}
par5_lda <- LDA(data_tm, k = 5, control = list(seed = 1234))
```

```{r}
par5_topics <- tidy(par5_lda, matrix = "beta")
```

#посмотрим на популярные слова в каждой теме - более менее к повестке подходит тема 5 (такие слова как язык, родной подходят к нашей теме). При этом в данных в целом было мало текстах чисто про язык, в основном были тексты про язык в связке с образованием. 
```{r message = FALSE}

par5_top_terms <- par5_topics %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

par5_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

#выберем только основную тему абзаца и оставим только те абзацы, которые относятся к нашей теме
```{r}
par5_documents <- tidy(par5_lda, matrix = "gamma")
preferredTopic = par5_documents %>% group_by(document) %>% arrange(-gamma) %>% 
  slice(1)
preferredTopic$document <- as.integer(preferredTopic$document)
listperson <- data %>% select(document, name, text)
topic5 <- preferredTopic  %>% filter(topic == 5) %>% 
  inner_join(listperson, by = "document")
```

#посмотрим, какой процент абзацев с данной темой у каждого спикера  - больше всех на тему языка (согласно данной модели) высказывались Гильмутдинов И.И., Гумерова Л.С, Аиткулова Э.Р., Калимуллин Р.Г.
```{r}
par_lang <- topic5 %>% group_by(name) %>% count() 
par_all <- data %>% group_by(name) %>% count()
par <- par_lang %>% inner_join(par_all, by="name")
par <- par %>% mutate(perc=n.x*100/n.y)
par %>% select(name,perc) %>% arrange(-perc)
```
Гильмутдинов высказывался на тему языка. Гумерова высказывалась на тему языка (но основная повестка - образование - тоже затрагивается в выбранной третьей теме). Аиткулова высказывалась на тему языка. Калимуллин не высказывался на тему языка - при этом высказывался на тему образования - и скорее всего поэтому попал в данную тему. 

### Биграммы 
```{r}
par.bigrams = data %>% 
  unnest_tokens(bigram, lem, token = "ngrams", n = 2)

par.bifiltered = par.bigrams %>% 
  separate(bigram, c("word1", "word2"), sep = " ") %>% 
  dplyr::filter(!word1 %in% rustopwords$words) %>% 
  dplyr::filter(!word2 %in% rustopwords$words) 

par.bifiltered = par.bifiltered %>% 
  unite(bigram, word1, word2, sep = " ")

bigrams_counts = par.bifiltered %>% 
  dplyr::count(document,bigram, sort = TRUE)
```

#по биграммам оптимальное количество тем - 5
```{r}
data_bi <- bigrams_counts %>%
  cast_dtm(document, bigram, n)

result_bi <- FindTopicsNumber(
  data_bi,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result_bi)
```
#построим модель с 5 темами
```{r}
par_lda_bi <- LDA(data_bi, k = 5, control = list(seed = 1234))
par_topics_bi <- tidy(par_lda_bi, matrix = "beta")
```

#при этом темы плохо выделяются в данном диапазоне
```{r message = FALSE}

par_top_terms_bi <- par_topics_bi %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

par_top_terms_bi %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```
#И слова и биграммы 
```{r}
names(bigrams_counts)[names(bigrams_counts) == 'bigram'] <- 'words'
word_bi_counts <- rbind(word_counts, bigrams_counts)
```

```{r}
data_bi_words <- word_bi_counts %>%
  cast_dtm(document, words, n)
```

#по словам и биграммам вместе оптимальное количество тем - 6
```{r}
result_bi_words <- FindTopicsNumber(
  data_bi_words,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)

FindTopicsNumber_plot(result_bi_words)
```
#построим модель с 6 темами
```{r}
par_lda_bi_words <- LDA(data_bi_words, k = 6, control = list(seed = 1234))
par_topics_bi_words <- tidy(par_lda_bi_words, matrix = "beta")
```

#модель лучше, чем просто  биграммы, но не лучше, чем модель со словами - здесь несколько тем связанных, с языком выделяются в три темы из пяти (3,4,5)
```{r message = FALSE}

words_top_terms <- par_topics_bi_words %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

words_top_terms %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

### Документ целиком 
```{r}
library(readxl)
path <- c("~/yandex/topic_modeling_doc.xlsx")
data_full <- read_excel(path)
document = c(1:138)
data_full = cbind(data_full, document)
data_full = data_full %>% dplyr::select(-number)
```

#лемматизация 
```{r}
path <- c("~/yandex/lem.xlsx")
data_lem  <- read_excel(path)
```
```{r}
data_lem <- data_lem %>% dplyr::select(2)
names(data_lem)[names(data_lem) == '0'] <- 'lem'
data_full <- cbind(data_full, data_lem)
```

#удаляем числа и пунктуацию, приводим слова к нижнему регистру 
```{r}
data_full$lem = str_to_lower(data_full$lem)
data_full$lem = str_remove_all(data_full$lem, punct) 
data_full$lem= str_remove_all(data_full$lem, numbers) 
```

#разбивка на токены и группировка по словам
```{r}
data_full_tokens = data_full %>%
  unnest_tokens(words, lem) %>% anti_join(rustopwords)

word_counts_full <- data_full_tokens %>%
  count(document, words, sort = TRUE) %>%
  ungroup()
```

#выберем оптимальное количество тем 
```{r}
data_tm_full <- word_counts_full %>%
  cast_dtm(document, words, n)

result_full <- FindTopicsNumber(
  data_tm_full,
  topics = seq(from = 2, to = 15, by = 1),
  metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
  method = "Gibbs",
  control = list(seed = 77),
  mc.cores = 2L,
  verbose = TRUE
)
```

#если смотреть на метрики, то можно сказать, что 5 - самое приемлемое количество тем
```{r}
FindTopicsNumber_plot(result_full)
```


#построение модели - мы выбрали 5 тем 
```{r}
lda_full <- LDA(data_tm_full, k = 3, control = list(seed = 1234))
topics_full <- tidy(lda_full, matrix = "beta")
```

#посмотрим на популярные слова в каждой теме - более менее к повестке подходит тема под номером 1 и 5
```{r message = FALSE}

top_terms_full <- topics_full %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms_full %>%
  mutate(term = reorder(term, beta)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()
```

#выбираем только 3-ю тему - те тексты, тема в которых составляет 60 и больше процентов 
```{r}
documents_full <- tidy(lda_full, matrix = "gamma")
documents_full <- documents_full %>% filter(topic==2)
documents_full <- documents_full %>% filter(gamma>=0.6)
data_filt <- data_full %>% select(name, document)
documents_full$document <- as.integer(documents_full$document)
documents_full <- documents_full %>% inner_join(data_filt, by="document")
```

#больше всех на тему языка высказывались (согласно модели) Гильмутдинов И.И., Умаханов И.М, Аиткулова Э.Р., Калимуллин Р.Г. 
```{r}
full_topic <- documents_full %>% group_by(name) %>% count() 
full_all <- data_full %>% group_by(name) %>% count()
full_perc <- full_topic %>% inner_join(full_all, by="name")
full_perc <- full_perc %>% mutate(perc=n.x*100/n.y)
full_perc %>% select(name,perc) %>% arrange(-perc)
```

Гильмутдинов высказывался на тему языка. Умаханов также высказывался на тему языка (в основном о мультикультурализме). Аиткулова высказывалась на тему языка. Гумерова высказывалась на тему языка. У Гасанова всего 5 текстов. Таким образом, можно сказать, что наиболее точные результаты по Гильмутдинову и Аиткуловой - в абсолютных значениях и в процентном соотношении. И он, и она высказывались на тему языка. То есть можно сказать, что модель нормально работает. 

#STM 

```{r}
data_dfm <- quanteda::corpus(data$lem)
data_dfm  <- tokens(data_dfm,
                   what = "word",
                   remove_symbols = T,
                   include_docvars = T) %>%
  dfm() %>%
  dfm_wordstem() %>%
  dfm_select(pattern = rustopwords$words,
             selection = c("remove"),
             valuetype = c("fixed"))
quanteda::meta(data_dfm) <- data
```

```{r}
data_stm <- convert(data_dfm,
                          to = c("stm"),
                          docvars = data)
```

```{r}
Bigmeta_og <- data_stm$meta
Bigvocab_og <- data_stm$vocab
Bigdocs_og <- data_stm$documents
```

```{r}
Big_outprep <- prepDocuments(Bigdocs_og,
                             Bigvocab_og,
                             Bigmeta_og)
```


```{r}
BigSTM_manyT <- manyTopics(documents = Big_outprep$documents,
                           vocab = Big_outprep$vocab,
                           K = c(5, 10, 15),
                           data = Big_outprep$meta,
                           verbose = F,
                           init.type = c("Spectral"), seed=1234)
```

```{r}
par(mar = c(5, 4, 2, 1),
    oma = c(0.8, 0.8, 0.8, 0.5),
    cex = 0.8)
plot(BigSTM_manyT$semcoh[[1]],
     BigSTM_manyT$exclusivity[[1]],
     col = "blue",
     pch = 19,
     xlab = c(" "),
     ylab = c(" "),
     xlim = c(-280, -40),
     ylim = c(9.4, 10.1)
)
points(BigSTM_manyT$semcoh[[2]],
       BigSTM_manyT$exclusivity[[2]],
       col = "green",
       pch = 19
)
points(BigSTM_manyT$semcoh[[3]],
       BigSTM_manyT$exclusivity[[3]],
       col = "red",
       pch = 19
)
legend("bottomleft",
       legend = c("5 Topics","10 Topics","15 Topics"),
       fill = c("blue","green","red"),
       title = c("Models"))
title(main = c("Topic Quality"),
      xlab = c("Semantic Coherence"),
      ylab = c("Exclusivity"))
```
```{r}
BigSTM_5 <- BigSTM_manyT$out[[1]]
BigSTM_10 <- BigSTM_manyT$out[[2]]
BigSTM_15 <- BigSTM_manyT$out[[3]]
```

```{r}
checkResiduals(BigSTM_5, documents = Big_outprep$documents)
```

```{r}
checkResiduals(BigSTM_10, documents = Big_outprep$documents)
```
```{r}
checkResiduals(BigSTM_15, documents = Big_outprep$documents)
```
#модель с 15-ю темами лучше 
#14-я тема связана с языком 
```{r}
summary(BigSTM_15)
```

```{r}
topics_stm <- tidy(BigSTM_15, matrix = "gamma")
preferredTopic_stm = topics_stm  %>% group_by(document) %>% arrange(-gamma) %>% 
  slice(1)
preferredTopic_stm$document <- as.integer(preferredTopic_stm$document)
topic14 <- preferredTopic_stm  %>% filter(topic == 14) %>% 
  inner_join(listperson, by = "document")
```


#посмотрим, какой процент абзацев с данной темой у каждого спикера  - больше всех на тему языка (согласно данной модели) высказывались  Жамсуев Б.Б, Гасанов Д.Н., Гильмутдинов И.И. и Аиткулова Э.Р.
```{r}
par_stm <- topic14 %>% group_by(name) %>% count() 
par_stm <- par_stm %>% inner_join(par_all, by="name")
par_stm <- par_stm %>% mutate(perc=n.x*100/n.y)
par_stm %>% select(name,perc) %>% arrange(-perc)
```
У Гильмутдинова и Аиткуловой была языковая повестка. Жамсуев высказывался про язык (но про английский и русский). У Гасанова слишком мало текстов. В целом можно сказать, что данная модель более точная (специфичная). 
